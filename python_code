pip install -U transformers
pip install -U accelerate
pip install -U datasets
pip install -U bertviz
pip install -U umap-learn
pip install -U sentencepiece
pip install -U urllib3
pip install -U py7zr
from datasets import load_dataset
dataset = load_dataset("cnn_dailymail","3.0.0")
dataset
dataset['train']
dataset['train'][1]['article'][:350]
dataset['train'][1]['highlights']
from transformers import pipeline
pipe = pipeline("text-generation",model="gpt2-medium")
dataset['train'][1]['article'][:2000]
input_text = dataset['train'][1]['article'][:2000]
query = input_text + "\nTL;DR:\n"
pipe_out = pipe(query,max_length = 512,clean_up_tokenization_spaces=True)
pipe_out[0]['generated_text'][len(query):]
summaries={}
summaries['gpt2-medium-380M']=pipe_out
from transformers import pipeline
pipe = pipeline("summarization",model="t5-base")
pipe_out = pipe(input_text)
summaries['t5-base-223M']=pipe_out[0]['summary_text']
pipe = pipeline("summarization",model="facebook/bart-large-cnn")
pipe_out = pipe(input_text)
summaries['bart-large-cnn-400M'] = pipe_out[0]['summary_text']
for model in summaries:
  print(model.upper())
  print(summaries[model])
  print("")
  from datasets import load_dataset
from transformers import pipeline

from transformers import AutoModelForSeq2SeqLM,AutoTokenizer
import torch
device='gpu'
model_ckpt = 'facebook/bart-large-cnn'
tokenizer = AutoTokenizer.from_pretrained(model_ckpt)
model = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt)
samsum=load_dataset('samsum')
samsum
samsum['train'][0]
dialogue_len=[len(x['dialogue'].split())for x in samsum['train']]
summary_len=[len(x['summary'].split())for x in samsum['train']]
import pandas as pd
data = pd.DataFrame([dialogue_len,summary_len]).T
data.colums = ['Dialogue Length','Summary length']
data.hist(figsize=(15,5))
def get_feature(batch):
  encoding = tokenizer(batch['dialogue'],text_target=batch['summary'],max_length=1024,truncation=True)
  encoding ={'input_ids':encoding['input_ids'],'attention_mask':encoding['attention_mask'],'labels':encoding['labels']}

  return encoding
samsum_pt = samsum.map(get_feature,batched = True)
samsum_pt
columns = ['input_ids','labels','attention_mask']
samsum_pt.set_format(type='torch',columns=columns)
from transformers import DataCollatorForSeq2Seq
data_collator = DataCollatorForSeq2Seq(tokenizer,model=model)
from transformers import TrainingArguments,Trainer
training_agrs = TrainingArguments(
    output_dir = 'bart_samsum',
    num_train_epochs=1,
    warmup_steps=500,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    weight_decay=0.01,
    logging_steps = 10,
    evaluation_strategy = 'steps',
    eval_steps=500,
    save_steps=1e6,
    gradient_accumulation_steps=16
    )
trainer = Trainer(model=model,args=training_agrs,tokenizer=tokenizer,data_collator=data_collator,
                  train_dataset = samsum_pt['train'],eval_dataset = samsum_pt['validation'])
trainer.train()
trainer.save_model('Coderone_2ndProject')
pipe = pipeline('summarization', model='Coderone_2ndProject')
gen_kwargs = {'length_penalty': 0.8, 'num_return_sequences': 4, "max_length": 128}

custom_dialogue = """
We have looked, and we cannot find an American history study on a grammar-school level that we think rivals H. A. Guerber’s two-volume American history set from the turn of the 20th century. So, rather than settle for lesser quality, we have combined Guerber’s The Story of the Thirteen Colonies and the Great Republic into one volume that makes it a perfect one-year survey of American history for the middle school years. Our edition of this text has been heavily edited to make it more concise and to allow for the fact that many events considered pivotal to history in the late 1800s have paled in significance with the passing of time and events such as two world wars.
"""

print(pipe(custom_dialogue, **gen_kwargs))


